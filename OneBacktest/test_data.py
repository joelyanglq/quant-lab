"""
æµ‹è¯•æ•°æ®æ¨¡å—

éªŒè¯å®Œæ•´æ•°æ®æµï¼š
    ParquetStorage.save() â†’ ParquetFeed.subscribe() â†’ DataHandler.update() â†’ MarketEvent
"""
import sys
import os
import queue
import shutil

import numpy as np
import pandas as pd

# ç¡®ä¿å¯ä»¥ import é¡¹ç›®æ¨¡å—
sys.path.insert(0, os.path.dirname(__file__))

from data import ParquetStorage, HistoricFeed, DataHandler, Bar
from event import MarketEvent


def generate_test_data(storage: ParquetStorage):
    """ç”Ÿæˆæµ‹è¯•ç”¨çš„ Parquet æ•°æ®ï¼ˆæ¨¡æ‹Ÿ AAPL å’Œ SPY æ—¥çº¿ï¼‰"""
    dates = pd.bdate_range('2023-01-03', '2023-01-31')  # å·¥ä½œæ—¥

    # AAPL
    np.random.seed(42)
    aapl_close = 130.0 + np.cumsum(np.random.randn(len(dates)) * 2)
    aapl_df = pd.DataFrame({
        'open': aapl_close - np.random.rand(len(dates)),
        'high': aapl_close + np.abs(np.random.randn(len(dates))) * 1.5,
        'low': aapl_close - np.abs(np.random.randn(len(dates))) * 1.5,
        'close': aapl_close,
        'volume': np.random.randint(50_000_000, 100_000_000, len(dates)),
    }, index=dates)
    # ç¡®ä¿ high >= max(open, close) å’Œ low <= min(open, close)
    aapl_df['high'] = aapl_df[['open', 'high', 'close']].max(axis=1)
    aapl_df['low'] = aapl_df[['open', 'low', 'close']].min(axis=1)

    # SPY
    spy_close = 390.0 + np.cumsum(np.random.randn(len(dates)) * 3)
    spy_df = pd.DataFrame({
        'open': spy_close - np.random.rand(len(dates)),
        'high': spy_close + np.abs(np.random.randn(len(dates))) * 2,
        'low': spy_close - np.abs(np.random.randn(len(dates))) * 2,
        'close': spy_close,
        'volume': np.random.randint(80_000_000, 150_000_000, len(dates)),
    }, index=dates)
    spy_df['high'] = spy_df[['open', 'high', 'close']].max(axis=1)
    spy_df['low'] = spy_df[['open', 'low', 'close']].min(axis=1)

    storage.save('AAPL', aapl_df, '1d')
    storage.save('SPY', spy_df, '1d')
    print(f"âœ… ç”Ÿæˆæµ‹è¯•æ•°æ®: AAPL ({len(aapl_df)} bars), SPY ({len(spy_df)} bars)")
    return aapl_df, spy_df


def test_parquet_storage():
    """æµ‹è¯• 1: ParquetStorage è¯»å†™"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 1: ParquetStorage è¯»å†™")
    print("=" * 60)

    storage = ParquetStorage('./test_data_cache')
    aapl_df, spy_df = generate_test_data(storage)

    # è¯»å›æ¥éªŒè¯
    loaded = storage.load('AAPL', frequency='1d')
    assert len(loaded) == len(aapl_df), f"é•¿åº¦ä¸åŒ¹é…: {len(loaded)} vs {len(aapl_df)}"
    assert list(loaded.columns) == list(aapl_df.columns), "åˆ—ä¸åŒ¹é…"

    # æ—¥æœŸèŒƒå›´è¿‡æ»¤
    start = pd.Timestamp('2023-01-10')
    end = pd.Timestamp('2023-01-20')
    filtered = storage.load('AAPL', start, end, '1d')
    assert all(filtered.index >= start), "è¿‡æ»¤å¤±è´¥: æœ‰æ—©äº start çš„æ•°æ®"
    assert all(filtered.index <= end), "è¿‡æ»¤å¤±è´¥: æœ‰æ™šäº end çš„æ•°æ®"
    print(f"âœ… æ—¥æœŸè¿‡æ»¤: {len(filtered)} bars in [{start.date()}, {end.date()}]")

    # exists
    assert storage.exists('AAPL', '1d'), "exists() åº”è¿”å› True"
    assert not storage.exists('GOOG', '1d'), "exists() åº”è¿”å› False"
    print("âœ… ParquetStorage æµ‹è¯•é€šè¿‡")


def test_historic_feed():
    """æµ‹è¯• 2: HistoricFeed Iterator"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: HistoricFeed Iterator")
    print("=" * 60)

    storage = ParquetStorage('./test_data_cache')
    feed = HistoricFeed(storage, frequency='1d')

    start = pd.Timestamp('2023-01-03')
    end = pd.Timestamp('2023-01-31')
    feed.subscribe(['AAPL', 'SPY'], start, end)

    # éªŒè¯æŒ‰æ—¶é—´é¡ºåºè¾“å‡º
    bars = []
    prev_ts = None
    while feed.has_next():
        bar = feed.next()
        bars.append(bar)
        if prev_ts is not None:
            assert bar.timestamp >= prev_ts, f"æ—¶é—´çº¿ä¹±åº: {prev_ts} -> {bar.timestamp}"
        prev_ts = bar.timestamp

    print(f"âœ… æ€»å…±æ¶ˆè´¹ {len(bars)} bars")
    print(f"   æ—¶é—´èŒƒå›´: {bars[0].timestamp.date()} â†’ {bars[-1].timestamp.date()}")

    # éªŒè¯å¤šæ ‡çš„äº¤é”™
    symbols_seen = set()
    for bar in bars[:10]:
        symbols_seen.add(bar.symbol)
        print(f"   {bar.timestamp.date()} {bar.symbol:5s} close={bar.close:.2f}")
    assert len(symbols_seen) == 2, "å‰ 10 ä¸ª bar åº”åŒ…å«ä¸¤ä¸ªæ ‡çš„"
    print("âœ… å¤šæ ‡çš„æ—¶é—´çº¿è‡ªåŠ¨åˆå¹¶ã€æŒ‰æ—¶é—´æ’åº")
    print("âœ… HistoricFeed æµ‹è¯•é€šè¿‡")


def test_data_handler():
    """æµ‹è¯• 3: DataHandler æ··åˆè®¾è®¡ï¼ˆIterator + Queryï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: DataHandler (Iterator + Query)")
    print("=" * 60)

    events = queue.Queue()
    storage = ParquetStorage('./test_data_cache')
    feed = HistoricFeed(storage, frequency='1d')

    start = pd.Timestamp('2023-01-03')
    end = pd.Timestamp('2023-01-31')
    feed.subscribe(['AAPL', 'SPY'], start, end)

    handler = DataHandler(feed, events, cache_size=200)

    # æ¶ˆè´¹æ‰€æœ‰æ•°æ®
    count = 0
    while handler.update():
        count += 1

    print(f"âœ… handler.update() å…±è°ƒç”¨ {count} æ¬¡")

    # éªŒè¯äº‹ä»¶é˜Ÿåˆ—
    event_count = 0
    while not events.empty():
        event = events.get()
        assert isinstance(event, MarketEvent), f"æœŸæœ› MarketEvent, å¾—åˆ° {type(event)}"
        event_count += 1
    assert event_count == count, f"äº‹ä»¶æ•°é‡ä¸åŒ¹é…: {event_count} vs {count}"
    print(f"âœ… äº‹ä»¶é˜Ÿåˆ—æ”¶åˆ° {event_count} ä¸ª MarketEvent")

    # æµ‹è¯•æŸ¥è¯¢æ¥å£
    latest_1 = handler.get_latest_bar('AAPL')
    assert latest_1 is not None, "get_latest_bar åº”è¿”å› Bar"
    print(f"âœ… get_latest_bar('AAPL'): {latest_1.timestamp.date()} close={latest_1.close:.2f}")

    latest_5 = handler.get_latest_bars('AAPL', N=5)
    assert len(latest_5) == 5, f"æœŸæœ› 5 ä¸ª bar, å¾—åˆ° {len(latest_5)}"
    # éªŒè¯æ—¶é—´æ­£åº
    for i in range(1, len(latest_5)):
        assert latest_5[i].timestamp >= latest_5[i-1].timestamp, "get_latest_bars åº”è¿”å›æ—¶é—´æ­£åº"
    print(f"âœ… get_latest_bars('AAPL', 5): {[b.timestamp.date() for b in latest_5]}")

    # æµ‹è¯• DataFrame æŸ¥è¯¢
    df = handler.get_latest_bars_df('SPY', N=3)
    assert len(df) == 3, f"æœŸæœ› 3 è¡Œ, å¾—åˆ° {len(df)}"
    assert 'close' in df.columns, "DataFrame åº”åŒ…å« close åˆ—"
    print(f"âœ… get_latest_bars_df('SPY', 3):\n{df[['close', 'volume']]}")

    # éªŒè¯ continue_backtest
    assert not handler.continue_backtest, "æ•°æ®è€—å°½å continue_backtest åº”ä¸º False"
    print("âœ… DataHandler æµ‹è¯•é€šè¿‡")


def test_bar_validation():
    """æµ‹è¯• 4: Bar æ•°æ®éªŒè¯"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: Bar æ•°æ®éªŒè¯")
    print("=" * 60)

    # æ­£å¸¸ bar
    bar = Bar(
        timestamp=pd.Timestamp('2023-01-03'),
        symbol='AAPL',
        open=130.0, high=132.0, low=128.0, close=131.0, volume=50000000
    )
    print(f"âœ… æ­£å¸¸ Bar: {bar.symbol} {bar.timestamp.date()} O={bar.open} H={bar.high} L={bar.low} C={bar.close}")

    # high < low åº”æŠ¥é”™
    try:
        Bar(timestamp=pd.Timestamp('2023-01-03'), symbol='BAD',
            open=130.0, high=125.0, low=128.0, close=131.0, volume=1000)
        assert False, "åº”è¯¥æŠ›å‡º ValueError"
    except ValueError as e:
        print(f"âœ… æ‹’ç»éæ³• bar (high < low): {e}")

    # frozen ä¸å¯å˜
    try:
        bar.close = 999.0
        assert False, "åº”è¯¥æŠ›å‡º FrozenInstanceError"
    except AttributeError:
        print("âœ… Bar ä¸å¯å˜ (frozen)")

    print("âœ… Bar æ•°æ®éªŒè¯æµ‹è¯•é€šè¿‡")


def cleanup():
    """æ¸…ç†æµ‹è¯•æ•°æ®"""
    if os.path.exists('./test_data_cache'):
        shutil.rmtree('./test_data_cache')
        print("\nğŸ§¹ æ¸…ç†æµ‹è¯•æ•°æ®ç›®å½•")


if __name__ == '__main__':
    try:
        test_bar_validation()
        test_parquet_storage()
        test_historic_feed()
        test_data_handler()
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("=" * 60)
    finally:
        cleanup()
